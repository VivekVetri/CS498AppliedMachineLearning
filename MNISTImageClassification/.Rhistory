names(getModelInfo())
model2 <- caret::train(x = train[,-c(9)], y= train[,c(9)], method ="naive_bayes", trControl=trainControl(method='cv',number=10)  )
model2 <- caret::train(x = train[,-c(9)], y= train[,c(9)], method ="naive_bayes", trControl=trainControl(method='cv',number=10)  )
model2 <- caret::train(x = train[,-c(9)], y= train[,c(9)], method ="nb", trControl=trainControl(method='cv',number=10)  )
model2 <- caret::train(x = train[,-c(9)], y= train[,c(9)], method ="nb", trControl=trainControl(method='cv',number=10)  )
model2 <- caret::train(x = train[,-c(9)], y= train[,c(9)],"nb", trControl=trainControl(method='cv',number=10)  )
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
predict(model2, test[,-c(9)])
pred2 <- predict(model2, test[,-c(9)])
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"naive_bayes", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
pred2 <- predict(model2, test[,-c(9)])
pred2
test[,9]
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"naive_bayes", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
gotright = test[,9] ==pred2$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
gotright = test[,9] ==pred2
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
accuracy
gotright = test[,9] ==pred1$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
pred1 <- predict(object = model1, test[,-c(9)])
gotright = test[,9] ==pred1$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
accuracy
svmModel <- svmlight(x = train[,-c(9)], y= as.factor(train[,c(9)]))
svmModel <- svmlight(train[,-c(9)], as.factor(train[,c(9)]))
svmModel <- svmlight(train[,-c(9)], as.factor(train[,c(9)]), pathsvm='Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/')
svmModel <-  svmlight(V9 ~ ., data=train)
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/')
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/svm_classify')
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/svm_classify')
setwd("Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/")
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='')
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='svm_classify')
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='svm_learn')
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='_train_1.dat')
bigx[, 1]==0
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
gotright = test[,9] ==pred2
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
accuracy
svmModel <-  svmlight(train[,-c(9)], as.factor(train[,c(9)]), data=train, pathsvm='_train_1.dat')
dataset = datasets.fetch_mldata("MNIST Original")
install.packages("dataset")
install.packages("datasets")
install.packages("datasets")
install.packages("datasets")
library(datasets)
dataset = datasets.fetch_mldata("MNIST Original")
install.packages("darch")
library(darch)
to.read = file("../CS498AppliedMachineLearning/MNIST/train-images-idx3-ubyte", "rb")
to.read
readBin(to.read, integer(), n=1, endian="big")
readBin(to.read, integer(), n=1, endian="big")
readBin(to.read, integer(), n=1, endian="big")
# load image files
load_image_file = function(filename) {
ret = list()
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
close(f)
data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}
# load label files
load_label_file = function(filename) {
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
close(f)
y
}
setwd("../CS498AppliedMachineLearning/MNIST/")
load_image_file = function(filename) {
ret = list()
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
close(f)
data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}
load_label_file = function(filename) {
f = file(filename, 'rb')
readBin(f, 'integer', n = 1, size = 4, endian = 'big')
n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
close(f)
y
}
train = load_image_file("train-images-idx3-ubyte")
test  = load_image_file("t10k-images-idx3-ubyte")
train
train$y = as.factor(load_label_file("train-labels-idx1-ubyte"))
test$y  = as.factor(load_label_file("t10k-labels-idx1-ubyte"))
pixel_header = function(x)
{
out = array()
for (ix in 1:x)
{
out[ix] = sprintf("pixel%i", ix-1)
}
out[ix+1] = "class"
return (out)
}
ph = pixel_header(784) #adds "class" at [785]
names(train) = ph      #sets header of training set data frame
names(test) = ph       #sets header of test set data frame
write.csv(train, file="mnist_train.csv", row.names=FALSE)
write.csv(test, file="mnist_test.csv", row.names=FALSE)
head(train)
28*28
install.packages("imager")
library(imager)
svmModel <-  svmlight(train[,-c(9)], as.factor(train[,c(9)]), data=train, pathsvm='_train_1.dat')
svmModel <-  svmlight(train[,-c(9)], as.factor(train[,c(9)]), data=train, pathsvm='_train_1.dat')
svmModel <-  svmlight(train[,-c(9)], as.factor(train[,c(9)], pathsvm='_train_1.dat')
svmModel <-  svmlight(train[,-c(9)], as.factor(train[,c(9)]), pathsvm='_train_1.dat')
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
library(klaR)
library(caret)
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
library(klaR)
library(caret)
wdat <- read.table("Desktop/Applied Machine Learning/CS498AppliedMachineLearning/PrimaIndians/pima-indians-diabetes.data", sep = ",")
# Source: http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/
# Col names of the data set
# 1. Number of times pregnant
# 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test
# 3. Diastolic blood pressure (mm Hg)
# 4. Triceps skin fold thickness (mm)
# 5. 2-Hour serum insulin (mu U/ml)
# 6. Body mass index (weight in kg/(height in m)^2)
# 7. Diabetes pedigree function
# 8. Age (years)
# 9. Class variable (0 or 1)
bigx<-wdat[,-c(9)]
bigy<-wdat[,9]
trscore <- array(dim = 10)
tescore <- array(dim = 10)
#####################
#Part A
#############
for (wi in 1:10) {
wtd <- createDataPartition(y = bigy, p = 0.8, list = FALSE) # 80% of the data into training
nbx <- bigx                                 # matrix of features
ntrbx <- nbx[wtd, ]                         # training features
ntrby <- bigy[wtd]                          # training labels
trposflag <- ntrby > 0                      # training labels for diabetes positive
ptregs <- ntrbx[trposflag, ]                # training rows features with diabetes positive
ntregs <- ntrbx[!trposflag, ]               # training rows features with diabetes negative
ntebx <- nbx[-wtd, ]                        # test rows - features
nteby <- bigy[-wtd]                         # test rows - labels
ptrmean <- sapply(ptregs, mean, na.rm = T)  # vector of means for training, diabetes positive
ntrmean <- sapply(ntregs, mean, na.rm = T)  # vector of means for training, diabetes negative
ptrsd   <- sapply(ptregs, sd, na.rm = T)    # vector of sd for training, diabetes positive
ntrsd   <- sapply(ntregs, sd, na.rm = T)    # vector of sd for training, diabetes negative
ptroffsets <- t(t(ntrbx) - ptrmean)         # first step normalize training diabetes pos, subtract mean
ptrscales  <- t(t(ptroffsets) / ptrsd)      # second step normalize training diabetes pos, divide by sd
ptrlogs    <- -(1/2) * rowSums(apply(ptrscales, c(1,2),
function(x) x^2), na.rm = T) - sum(log(ptrsd))+log(NROW(ptregs)/NROW(ntrby))  # Log likelihoods based on
# normal distr. for diabetes positive
ntroffsets <- t(t(ntrbx) - ntrmean)
ntrscales  <- t(t(ntroffsets) / ntrsd)
ntrlogs    <- -(1/2) * rowSums(apply(ntrscales, c(1,2)
, function(x) x^2), na.rm = T) - sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
# Log likelihoods based on
# normal distr for diabetes negative
# (It is done separately on each class)
lvwtr      <- ptrlogs > ntrlogs              # Rows classified as diabetes positive by classifier
gotrighttr <- lvwtr == ntrby                 # compare with true labels
trscore[wi]<- sum(gotrighttr)/(sum(gotrighttr)+sum(!gotrighttr)) # Accuracy with training set
pteoffsets <- t(t(ntebx)-ptrmean)            # Normalize test dataset with parameters from training
ptescales  <- t(t(pteoffsets)/ptrsd)
ptelogs    <- -(1/2)*rowSums(apply(ptescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ptrsd)) +log(NROW(ptregs)/NROW(ntrby))
nteoffsets <- t(t(ntebx)-ntrmean)            # Normalize again for diabetes negative class
ntescales  <- t(t(nteoffsets)/ntrsd)
ntelogs    <- -(1/2)*rowSums(apply(ntescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
lvwte<-ptelogs>ntelogs
gotright<-lvwte==nteby
tescore[wi]<-sum(gotright)/(sum(gotright)+sum(!gotright))  # Accuracy on the test set
}
print(tescore)
print(mean(tescore))
#########
#Part B
########
wdat2 <-wdat
#replace 0 to NA
wdat2$V3[wdat2$V3 == 0] <- NA
wdat2$V4[wdat2$V4 == 0] <- NA
wdat2$V6[wdat2$V6 == 0] <- NA
wdat2$V8[wdat2$V8 == 0] <- NA
bigx<-wdat2[,-c(9)]
bigy<-wdat2[,9]
trscore <- array(dim = 10)
tescore <- array(dim = 10)
for (wi in 1:10) {
wtd <- createDataPartition(y = bigy, p = 0.8, list = FALSE) # 80% of the data into training
nbx <- bigx                                 # matrix of features
ntrbx <- nbx[wtd, ]                         # training features
ntrby <- bigy[wtd]                          # training labels
trposflag <- ntrby > 0                      # training labels for diabetes positive
ptregs <- ntrbx[trposflag, ]                # training rows features with diabetes positive
ntregs <- ntrbx[!trposflag, ]               # training rows features with diabetes negative
ntebx <- nbx[-wtd, ]                        # test rows - features
nteby <- bigy[-wtd]                         # test rows - labels
ptrmean <- sapply(ptregs, mean, na.rm = T)  # vector of means for training, diabetes positive
ntrmean <- sapply(ntregs, mean, na.rm = T)  # vector of means for training, diabetes negative
ptrsd   <- sapply(ptregs, sd, na.rm = T)    # vector of sd for training, diabetes positive
ntrsd   <- sapply(ntregs, sd, na.rm = T)    # vector of sd for training, diabetes negative
ptroffsets <- t(t(ntrbx) - ptrmean)         # first step normalize training diabetes pos, subtract mean
ptrscales  <- t(t(ptroffsets) / ptrsd)      # second step normalize training diabetes pos, divide by sd
ptrlogs    <- -(1/2) * rowSums(apply(ptrscales, c(1,2),
function(x) x^2), na.rm = T) - sum(log(ptrsd))+log(NROW(ptregs)/NROW(ntrby))  # Log likelihoods based on
# normal distr. for diabetes positive
ntroffsets <- t(t(ntrbx) - ntrmean)
ntrscales  <- t(t(ntroffsets) / ntrsd)
ntrlogs    <- -(1/2) * rowSums(apply(ntrscales, c(1,2)
, function(x) x^2), na.rm = T) - sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
# Log likelihoods based on
# normal distr for diabetes negative
# (It is done separately on each class)
lvwtr      <- ptrlogs > ntrlogs              # Rows classified as diabetes positive by classifier
gotrighttr <- lvwtr == ntrby                 # compare with true labels
trscore[wi]<- sum(gotrighttr)/(sum(gotrighttr)+sum(!gotrighttr)) # Accuracy with training set
pteoffsets <- t(t(ntebx)-ptrmean)            # Normalize test dataset with parameters from training
ptescales  <- t(t(pteoffsets)/ptrsd)
ptelogs    <- -(1/2)*rowSums(apply(ptescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ptrsd)) +log(NROW(ptregs)/NROW(ntrby))
nteoffsets <- t(t(ntebx)-ntrmean)            # Normalize again for diabetes negative class
ntescales  <- t(t(nteoffsets)/ntrsd)
ntelogs    <- -(1/2)*rowSums(apply(ntescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
lvwte<-ptelogs>ntelogs
gotright<-lvwte==nteby
tescore[wi]<-sum(gotright)/(sum(gotright)+sum(!gotright))  # Accuracy on the test set
}
print(tescore)
print(mean(tescore))
###########
#Part C
###########
##Model with Klar
trainPartition <- createDataPartition(y = wdat[,9], p = 0.8, list = FALSE)
train <- wdat[trainPartition,]
test <-wdat[-trainPartition,]
model1 <- klaR::NaiveBayes(as.factor(V9)~.,data= train )
pred1 <- predict(object = model1, test[,-c(9)])
gotright = test[,9] ==pred1$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
gotright = test[,9] ==pred2
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
accuracy
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='_train_1.dat')
setwd("Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/")
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='../../svm_light_osx.8.4_i7/_train_1.dat')
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='../../svm_light_osx.8.4_i7')
svmModel
pred2 <- predict(svmModel, test[,-c(9)])
pred3 <- predict(svmModel, test[,-c(9)])
pred3
gotright = test[,9] ==pred3$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
accuracy
ntrby
sum(ntrby)
NROW(ptregs
)
sum(ptregs
)
ptregs
length(ptregs)
length(ptregs)
knitr::opts_chunk$set(echo = TRUE)
library(klaR)
library(caret)
wdat <- read.table("Desktop/Applied Machine Learning/CS498AppliedMachineLearning/PrimaIndians/pima-indians-diabetes.data", sep = ",")
setwd("../PrimaIndians/")
wdat <- read.table("Desktop/Applied Machine Learning/CS498AppliedMachineLearning/PrimaIndians/pima-indians-diabetes.data", sep = ",")
setwd("../PrimaIndians/")
wdat <- read.table("pima-indians-diabetes.data", sep = ",")
# Source: http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/
# Col names of the data set
# 1. Number of times pregnant
# 2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test
# 3. Diastolic blood pressure (mm Hg)
# 4. Triceps skin fold thickness (mm)
# 5. 2-Hour serum insulin (mu U/ml)
# 6. Body mass index (weight in kg/(height in m)^2)
# 7. Diabetes pedigree function
# 8. Age (years)
# 9. Class variable (0 or 1)
bigx<-wdat[,-c(9)]
bigy<-wdat[,9]
trscore <- array(dim = 10)
tescore <- array(dim = 10)
#####################
#Part A
#############
for (wi in 1:10) {
wtd <- createDataPartition(y = bigy, p = 0.8, list = FALSE) # 80% of the data into training
nbx <- bigx                                 # matrix of features
ntrbx <- nbx[wtd, ]                         # training features
ntrby <- bigy[wtd]                          # training labels
trposflag <- ntrby > 0                      # training labels for diabetes positive
ptregs <- ntrbx[trposflag, ]                # training rows features with diabetes positive
ntregs <- ntrbx[!trposflag, ]               # training rows features with diabetes negative
ntebx <- nbx[-wtd, ]                        # test rows - features
nteby <- bigy[-wtd]                         # test rows - labels
ptrmean <- sapply(ptregs, mean, na.rm = T)  # vector of means for training, diabetes positive
ntrmean <- sapply(ntregs, mean, na.rm = T)  # vector of means for training, diabetes negative
ptrsd   <- sapply(ptregs, sd, na.rm = T)    # vector of sd for training, diabetes positive
ntrsd   <- sapply(ntregs, sd, na.rm = T)    # vector of sd for training, diabetes negative
ptroffsets <- t(t(ntrbx) - ptrmean)         # first step normalize training diabetes pos, subtract mean
ptrscales  <- t(t(ptroffsets) / ptrsd)      # second step normalize training diabetes pos, divide by sd
ptrlogs    <- -(1/2) * rowSums(apply(ptrscales, c(1,2),
function(x) x^2), na.rm = T) - sum(log(ptrsd))+log(NROW(ptregs)/NROW(ntrby))  # Log likelihoods based on
# normal distr. for diabetes positive
ntroffsets <- t(t(ntrbx) - ntrmean)
ntrscales  <- t(t(ntroffsets) / ntrsd)
ntrlogs    <- -(1/2) * rowSums(apply(ntrscales, c(1,2)
, function(x) x^2), na.rm = T) - sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
# Log likelihoods based on
# normal distr for diabetes negative
# (It is done separately on each class)
lvwtr      <- ptrlogs > ntrlogs              # Rows classified as diabetes positive by classifier
gotrighttr <- lvwtr == ntrby                 # compare with true labels
trscore[wi]<- sum(gotrighttr)/(sum(gotrighttr)+sum(!gotrighttr)) # Accuracy with training set
pteoffsets <- t(t(ntebx)-ptrmean)            # Normalize test dataset with parameters from training
ptescales  <- t(t(pteoffsets)/ptrsd)
ptelogs    <- -(1/2)*rowSums(apply(ptescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ptrsd)) +log(NROW(ptregs)/NROW(ntrby))
nteoffsets <- t(t(ntebx)-ntrmean)            # Normalize again for diabetes negative class
ntescales  <- t(t(nteoffsets)/ntrsd)
ntelogs    <- -(1/2)*rowSums(apply(ntescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
lvwte<-ptelogs>ntelogs
gotright<-lvwte==nteby
tescore[wi]<-sum(gotright)/(sum(gotright)+sum(!gotright))  # Accuracy on the test set
}
print(tescore)
print(mean(tescore))
bigx<-wdat[,-c(9)]
bigy<-wdat[,9]
trscore <- array(dim = 10)
tescore <- array(dim = 10)
#####################
#Part A
#############
for (wi in 1:10) {
wtd <- createDataPartition(y = bigy, p = 0.8, list = FALSE) # 80% of the data into training
nbx <- bigx                                 # matrix of features
ntrbx <- nbx[wtd, ]                         # training features
ntrby <- bigy[wtd]                          # training labels
trposflag <- ntrby > 0                      # training labels for diabetes positive
ptregs <- ntrbx[trposflag, ]                # training rows features with diabetes positive
ntregs <- ntrbx[!trposflag, ]               # training rows features with diabetes negative
ntebx <- nbx[-wtd, ]                        # test rows - features
nteby <- bigy[-wtd]                         # test rows - labels
ptrmean <- sapply(ptregs, mean, na.rm = T)  # vector of means for training, diabetes positive
ntrmean <- sapply(ntregs, mean, na.rm = T)  # vector of means for training, diabetes negative
ptrsd   <- sapply(ptregs, sd, na.rm = T)    # vector of sd for training, diabetes positive
ntrsd   <- sapply(ntregs, sd, na.rm = T)    # vector of sd for training, diabetes negative
ptroffsets <- t(t(ntrbx) - ptrmean)         # first step normalize training diabetes pos, subtract mean
ptrscales  <- t(t(ptroffsets) / ptrsd)      # second step normalize training diabetes pos, divide by sd
ptrlogs    <- -(1/2) * rowSums(apply(ptrscales, c(1,2),
function(x) x^2), na.rm = T) - sum(log(ptrsd))+log(NROW(ptregs)/NROW(ntrby))  # Log likelihoods based on
# normal distr. for diabetes positive
ntroffsets <- t(t(ntrbx) - ntrmean)
ntrscales  <- t(t(ntroffsets) / ntrsd)
ntrlogs    <- -(1/2) * rowSums(apply(ntrscales, c(1,2)
, function(x) x^2), na.rm = T) - sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
# Log likelihoods based on
# normal distr for diabetes negative
# (It is done separately on each class)
lvwtr      <- ptrlogs > ntrlogs              # Rows classified as diabetes positive by classifier
gotrighttr <- lvwtr == ntrby                 # compare with true labels
trscore[wi]<- sum(gotrighttr)/(sum(gotrighttr)+sum(!gotrighttr)) # Accuracy with training set
pteoffsets <- t(t(ntebx)-ptrmean)            # Normalize test dataset with parameters from training
ptescales  <- t(t(pteoffsets)/ptrsd)
ptelogs    <- -(1/2)*rowSums(apply(ptescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ptrsd)) +log(NROW(ptregs)/NROW(ntrby))
nteoffsets <- t(t(ntebx)-ntrmean)            # Normalize again for diabetes negative class
ntescales  <- t(t(nteoffsets)/ntrsd)
ntelogs    <- -(1/2)*rowSums(apply(ntescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
lvwte<-ptelogs>ntelogs
gotright<-lvwte==nteby
tescore[wi]<-sum(gotright)/(sum(gotright)+sum(!gotright))  # Accuracy on the test set
}
print(tescore)
print(paste("Average Accuracy",mean(tescore)))
wdat2 <-wdat
#replace 0 to NA
wdat2$V3[wdat2$V3 == 0] <- NA
wdat2$V4[wdat2$V4 == 0] <- NA
wdat2$V6[wdat2$V6 == 0] <- NA
wdat2$V8[wdat2$V8 == 0] <- NA
bigx<-wdat2[,-c(9)]
bigy<-wdat2[,9]
trscore <- array(dim = 10)
tescore <- array(dim = 10)
for (wi in 1:10) {
wtd <- createDataPartition(y = bigy, p = 0.8, list = FALSE) # 80% of the data into training
nbx <- bigx                                 # matrix of features
ntrbx <- nbx[wtd, ]                         # training features
ntrby <- bigy[wtd]                          # training labels
trposflag <- ntrby > 0                      # training labels for diabetes positive
ptregs <- ntrbx[trposflag, ]                # training rows features with diabetes positive
ntregs <- ntrbx[!trposflag, ]               # training rows features with diabetes negative
ntebx <- nbx[-wtd, ]                        # test rows - features
nteby <- bigy[-wtd]                         # test rows - labels
ptrmean <- sapply(ptregs, mean, na.rm = T)  # vector of means for training, diabetes positive
ntrmean <- sapply(ntregs, mean, na.rm = T)  # vector of means for training, diabetes negative
ptrsd   <- sapply(ptregs, sd, na.rm = T)    # vector of sd for training, diabetes positive
ntrsd   <- sapply(ntregs, sd, na.rm = T)    # vector of sd for training, diabetes negative
ptroffsets <- t(t(ntrbx) - ptrmean)         # first step normalize training diabetes pos, subtract mean
ptrscales  <- t(t(ptroffsets) / ptrsd)      # second step normalize training diabetes pos, divide by sd
ptrlogs    <- -(1/2) * rowSums(apply(ptrscales, c(1,2),
function(x) x^2), na.rm = T) - sum(log(ptrsd))+log(NROW(ptregs)/NROW(ntrby))  # Log likelihoods based on
# normal distr. for diabetes positive
ntroffsets <- t(t(ntrbx) - ntrmean)
ntrscales  <- t(t(ntroffsets) / ntrsd)
ntrlogs    <- -(1/2) * rowSums(apply(ntrscales, c(1,2)
, function(x) x^2), na.rm = T) - sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
# Log likelihoods based on
# normal distr for diabetes negative
# (It is done separately on each class)
lvwtr      <- ptrlogs > ntrlogs              # Rows classified as diabetes positive by classifier
gotrighttr <- lvwtr == ntrby                 # compare with true labels
trscore[wi]<- sum(gotrighttr)/(sum(gotrighttr)+sum(!gotrighttr)) # Accuracy with training set
pteoffsets <- t(t(ntebx)-ptrmean)            # Normalize test dataset with parameters from training
ptescales  <- t(t(pteoffsets)/ptrsd)
ptelogs    <- -(1/2)*rowSums(apply(ptescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ptrsd)) +log(NROW(ptregs)/NROW(ntrby))
nteoffsets <- t(t(ntebx)-ntrmean)            # Normalize again for diabetes negative class
ntescales  <- t(t(nteoffsets)/ntrsd)
ntelogs    <- -(1/2)*rowSums(apply(ntescales,c(1, 2)
, function(x)x^2), na.rm=TRUE)-sum(log(ntrsd)) +log(NROW(ntregs)/NROW(ntrby))
lvwte<-ptelogs>ntelogs
gotright<-lvwte==nteby
tescore[wi]<-sum(gotright)/(sum(gotright)+sum(!gotright))  # Accuracy on the test set
}
print(tescore)
print(paste("Average Accuracy",mean(tescore)))
##Model with Klar
trainPartition <- createDataPartition(y = wdat[,9], p = 0.8, list = FALSE)
train <- wdat[trainPartition,]
test <-wdat[-trainPartition,]
model1 <- klaR::NaiveBayes(as.factor(V9)~.,data= train )
pred1 <- predict(object = model1, test[,-c(9)])
gotright = test[,9] ==pred1$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("accuracy using klar", accuracy))
#Model with caret
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
gotright = test[,9] ==pred2
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("accuracy using caret", accuracy))
##Model with Klar
trainPartition <- createDataPartition(y = wdat[,9], p = 0.8, list = FALSE)
train <- wdat[trainPartition,]
test <-wdat[-trainPartition,]
model1 <- klaR::NaiveBayes(as.factor(V9)~.,data= train )
pred1 <- predict(object = model1, test[,-c(9)])
gotright = test[,9] ==pred1$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("accuracy using klar", accuracy))
#Model with caret
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
gotright = test[,9] ==pred2
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("accuracy using caret", accuracy))
##Model with Klar
trainPartition <- createDataPartition(y = wdat[,9], p = 0.8, list = FALSE)
train <- wdat[trainPartition,]
test <-wdat[-trainPartition,]
model1 <- klaR::NaiveBayes(as.factor(V9)~.,data= train )
pred1 <- predict(object = model1, test[,-c(9)])
gotright = test[,9] ==pred1$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("accuracy using klar", accuracy))
#Model with caret
model2 <- caret::train(x = train[,-c(9)], y= as.factor(train[,c(9)]),"nb", trControl=trainControl(method='cv',number=10)  )
pred2 <- predict(model2, test[,-c(9)])
gotright = test[,9] ==pred2
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("accuracy using caret", accuracy))
setwd("Desktop/Applied Machine Learning/svm_light_osx.8.4_i7/")
svmModel <-  svmlight(V9 ~ ., data=train, pathsvm='../../svm_light_osx.8.4_i7/')
pred3 <- predict(svmModel, test[,-c(9)])
gotright = test[,9] ==pred3$class
accuracy =  sum(gotright)/(sum(gotright)+sum(!gotright))
print(paste("SVM accuracy", accuracy))
